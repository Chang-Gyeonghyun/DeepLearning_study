# FreeMatch: Self-adaptive Thresholding for Semi-supervised Learning

### 기존의 Semi-Supervised Learning (SSL)

기존의 SSL은 의사 라벨링(pseudo labeling) 및 일관성 정규화(consistency regularization)를 기반으로 한 지도 학습 방식을 통해 성과를 거두었다. 하지만 기존 방법들은 다음과 같은 한계가 있다:

1. **고정된 임계값 사용**: 의사 라벨의 품질을 보장하기 위해 미리 정해둔 고정된 높은 임계값을 사용한다. 이는 초기 학습 단계에서 라벨이 없는 데이터의 활용도를 낮춘다.
2. **임의의 임계값 조정**: AdaMatch는 학습이 진행됨에 따라 임계값을 점진적으로 증가시키지만, 이는 하이퍼파라미터에 의해 제어되며 모델의 학습 과정과는 연관이 없다.
3. **지역적 임계값**: FlexMatch는 각 클래스가 각기 다른 지역적 (클래스-특정) 임계값을 가져야 한다고 주장하지만, 이는 여전히 미리 정의된 고정된 글로벌 임계값에서 파생된 것이다.
    
![freematch](https://d3i71xaburhd42.cloudfront.net/a4c99a1f69909443b3ca895cdd3dc78070c03377/5-Figure2-1.png)
    

### FreeMatch의 자가 적응 임계값 설정 (SAT)

FreeMatch는 자가 적응 임계값 설정(Self-Adaptive Thresholding, SAT) 기술을 사용하여 라벨이 없는 데이터의 신뢰도의 지수 이동 평균(EMA)을 통해 글로벌(데이터셋-특정) 및 로컬(클래스-특정) 임계값을 추정한다.

### 글로벌 임계값(Global Threshold)

- **초기 값**: 학습 시작 시, 1/C(클래스 수)로 매우 낮게 설정한다.
- **업데이트**: EMA를 통해 임계값이 조금씩 업데이트된다. 과거의 값과 현재 모델이 각 샘플에 대해 예측한 값(가장 높게 예측한 클래스의 확률)의 평균으로 계산한다.

### 로컬 임계값(Local Threshold)

- **초기 값**: 글로벌 임계값과 마찬가지로 1/C로 시작한다.
- **업데이트**: 해당 클래스의 과거 로컬 임계값과 각 샘플별로 해당 클래스의 확률 값을 사용하여 EMA로 업데이트한다. 구해진 값을 MaxNorm 시킨 후 글로벌 임계값을 곱하여 최종 로컬 임계값을 구한다.

### 손실 함수 (Loss Function)

임계값을 구한 후, 손실 함수는 다음과 같이 계산한다. FixMatch의 손실 함수에서 고정된 임계값을 모델이 그 샘플에 대해 가장 높게 예측한 확률 값에 대한 로컬 임계값으로 대체한다.

### 의사 라벨 분포의 불균형 문제

학습을 진행할 때, 의사 라벨의 분포가 균형하지 않다는 문제가 있다. 맞추기 쉬운 클래스들은 상대적으로 먼저 임계값을 넘어 의사 라벨로 결정되며, 이렇게 되면 해당 클래스의 라벨이 다른 클래스보다 상대적으로 많아지게 된다. 이는 모델이 편향된 예측을 하도록 만든다.

### 균형 분포 손실 (Fairness Loss)

이를 해결하기 위해 Cross Entropy의 역수를 사용하여 특정 클래스가 아닌 분포 예측값들이 모호해지도록 만든다. 모든 클래스를 동일한 확률로 예측하도록 한다.

### 최종 손실 함수 (Final Loss)

라벨이 있는 데이터 손실(Labeled Data Loss), 라벨이 없는 데이터 손실(Unlabeled Data Loss), 균형 분포 손실(Fairness Loss)을 합친 것이 최종 손실이 된다.

### 결론

FreeMatch는 라벨 수가 적을 때 높은 성능을 보여준다.

학습 초기에 unlabeled data 활용도가 높아진다. 잘못된 pseudo labeling을 하더라도, 학습이 진행되는 과정에서 해당 class에 대한 확률이 낮아질 경우 임계값이 다시 조절이 된다.