### 기존 방법론의 문제점

기존의 의사 라벨링 방법들은 의사 라벨의 양이 증가하면 질이 떨어지고, 반대로 질이 높아지면 양이 줄어드는 문제가 있다.

SoftMatch는 훈련 중 샘플의 신뢰도에 기반한 가중치를 부여하는 절단된 가우시안 함수를 도입 + 균형 정렬 접근법을 제안했다.

신뢰도 기반 의사 라벨링의 핵심 아이디어는 예측 신뢰도가 일정 임계값 이상인 의사 라벨로 모델을 훈련시키고, 그렇지 않은 경우는 단순히 무시하는 것이다. 그러나 이러한 메커니즘은 본질적으로 양-질 trade-off를 나타내어 학습 과정을 저해한다. 

- FixMatch에서 활용된 높은 신뢰도 임계값은 의사 라벨의 품질을 보장하지만, 신뢰도가 낮지만 올바른 많은 의사 라벨을 버리게 된다.
- 동적으로 증가하는 임계값(FreeMatch)이나 클래스별 임계값(FlexMatch)은 더 많은 의사 라벨을 활용하도록 권장하지만, 불가피하게 오류가 있는 의사 라벨을 완전히 포함시켜 훈련을 방해할 수 있다. (FlexMatch에서 사용된 의사 라벨 중 약 16%가 잘못된 것이다.) 요약하면, 신뢰도 임계값을 통한 양-질 trade-off는 라벨링되지 않은 데이터의 활용을 제한하여 모델의 일반화 성능을 저해할 수 있다.

trade-off의 근본적인 이유는 임계값 이상인 의사 라벨이 모두 올바르다고 가정하는 반면, 그렇지 않은 라벨은 틀리다고 가정한다. 절단된 가우시안 함수를 사용하여 신뢰도 분포를 맞추고, 신뢰도의 평균에서 벗어나는 정도에 따라 가중치를 부여한다. 가우시안 함수의 매개변수는 훈련 중 모델의 과거 예측을 사용하여 추정된다. 더욱이, 클래스의 학습 난이도 차이로 인한 의사 라벨의 불균형 문제를 해결하기 위해 균형 정렬을 제안한다. 이는 의사 라벨의 양을 유지하면서 그들의 품질을 보장한다.

- **신뢰도 분포 분석**: 모델이 예측한 모든 샘플의 신뢰도를 분석한다. 이 신뢰도는 모델이 각 샘플에 대해 얼마나 확신을 가지고 있는지를 나타낸다.
- **가우시안 함수 적용**: 신뢰도의 평균(μ_t)과 표준 편차(σ_t)를 사용하여 가우시안 분포를 만든다. 이 분포에서 평균에 가까운 샘플들에 더 높은 가중치를 부여한다. 이는 신뢰도가 지나치게 높거나 낮은 샘플보다 평균에 가까운 샘플이 더 신뢰할 만하다는 가정을 기반으로 한다.
- **가중치 부여**: 가우시안 함수의 결과에 따라 각 샘플에 가중치를 부여한다. 즉, 신뢰도가 평균 신뢰도에 가까운 샘플일수록 높은 가중치를 받고, 평균에서 멀어질수록 가중치가 낮아진다.

### 왜 신뢰도가 높은 샘플에 낮은 가중치를 부여하는가?

신뢰도가 높은 샘플에 무조건 높은 가중치를 부여하는 것은 직관적일 수 있지만, SoftMatch는 이를 균형 있게 처리한다.

1. **과적합 방지**:
신뢰도가 높은 샘플은 모델이 이미 잘 학습한 샘플일 가능성이 크다. 이런 샘플에 높은 가중치를 계속 부여하면 모델이 이 샘플들에 과적합(overfitting)될 위험이 있다. 과적합은 모델의 일반화 성능을 저하시킬 수 있다.
2. **데이터 다양성 유지**:
신뢰도가 높은 샘플만 사용하면 다양한 샘플이 훈련에 반영되지 않을 수 있다. 이는 모델이 특정 패턴만 학습하게 되어 다양한 상황에서의 성능이 떨어질 수 있다. SoftMatch는 평균 신뢰도 근처의 샘플에도 가중치를 부여함으로써 다양한 데이터를 학습에 포함시킨다.

### 균형 정렬

샘플 가중치를 계산할 때, 균형 정렬은 덜 예측된 의사 라벨에는 더 큰 가중치를, 더 많이 예측된 의사 라벨에는 더 작은 가중치를 부여하여 불균형 문제를 완화한다.
각 클래스의 예측 확률을 EMA로 계산하여, 균등 분포와의 비율을 통해 예측 확률을 정규화하고 샘플 가중치를 계산.

덜 예측된 클래스의 의사 라벨에는 더 큰 가중치를 부여하고, 많이 예측된 클래스의 의사 라벨에는 더 작은 가중치를 부여하여 클래스 불균형 문제를 해결.

### 최종 Loss

$L = L_s + L_s$

$L_s = \frac{1}{B_L} \sum_{i=1}^{B_L} H(y_i, p(y|x_{li})) + L_u = \frac{1}{B_U} \sum_{i=1}^{B_U} \lambda(p_i) H(\hat{p}_i, p(y|\Omega(x_{ui})))$ 

$\lambda(p_i)$ 는 SoftMatch의 최종 샘플 가중치 함수로, 가우시안 함수와 Uniform Alignment(UA)가 모두 반영된 값.

### 왜 SoftMatch가 잘 작동하는가?

SoftMatch의 접근 방식이 잘 작동하는 이유는 다음과 같다:

1. **균형 잡힌 학습**:
SoftMatch는 신뢰도가 높은 샘플과 낮은 샘플 모두를 균형 있게 포함시킨다. 이는 모델이 다양한 샘플에 대해 학습할 수 있게 하여, 다양한 상황에서의 성능을 향상시킨다.
2. **효과적인 정보 활용**:
신뢰도 분포를 기반으로 가중치를 부여하여, 모델이 확신을 가지고 있는 샘플뿐만 아니라 약간의 불확실성을 가진 샘플도 학습에 포함한다. 이는 모델이 더 나은 일반화 능력을 가지도록 한다.
3. **동적 가중치 적용**:
SoftMatch는 절단된 가우시안 함수를 사용하여 동적으로 가중치를 부여한다. 이는 훈련 과정에서 모델의 성능 변화를 반영하여 적응적으로 학습을 조정할 수 있게 한다.
4. **불균형 데이터 처리**:
SoftMatch는 데이터의 불균형 문제를 해결하는 데 도움을 준다. 균형 정렬(Uniform Alignment) 기법을 통해 다양한 클래스의 샘플이 고르게 학습에 포함될 수 있도록 한다.